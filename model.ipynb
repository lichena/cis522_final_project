{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d082571acd024f40aea110112c1d068a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_52dfb7c37ab64d769d14648269b3a509",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09acf27640d649ada4a78a230806a8a5",
              "IPY_MODEL_d4435b723ba24ffcabc459d8d4f060a4"
            ]
          }
        },
        "52dfb7c37ab64d769d14648269b3a509": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09acf27640d649ada4a78a230806a8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_53f47255bc03483195470c256602e200",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 435778770,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435778770,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38add29bf33640fd8ef4d5dfb59f0041"
          }
        },
        "d4435b723ba24ffcabc459d8d4f060a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3500b1e4daca4ea187d8c4da54498b03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:38&lt;00:00, 11.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d81d3d6ae4b485099036b75d6250b39"
          }
        },
        "53f47255bc03483195470c256602e200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38add29bf33640fd8ef4d5dfb59f0041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3500b1e4daca4ea187d8c4da54498b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d81d3d6ae4b485099036b75d6250b39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4DsnCj7ulia",
        "colab_type": "code",
        "outputId": "9530647b-d09f-4a33-a68d-0921d0ba42b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "# setup drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHKZP9t2utIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-rys2hYWjYY",
        "colab_type": "code",
        "outputId": "5ea69206-a75d-4a49-9819-c0492a0d119d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 24.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 3.4MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 61.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 56.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=f08897a434b26f6964279c2ad6fdd09be528cebce59168baf7bd6823c2fbb490\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRk3kLMkWogx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d082571acd024f40aea110112c1d068a",
            "52dfb7c37ab64d769d14648269b3a509",
            "09acf27640d649ada4a78a230806a8a5",
            "d4435b723ba24ffcabc459d8d4f060a4",
            "53f47255bc03483195470c256602e200",
            "38add29bf33640fd8ef4d5dfb59f0041",
            "3500b1e4daca4ea187d8c4da54498b03",
            "0d81d3d6ae4b485099036b75d6250b39"
          ]
        },
        "outputId": "bfdd0a20-67a4-452b-e117-cadbe5de0f51"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model.eval()\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "  model = nn.DataParallel(model)\n",
        "model.to(device);"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d082571acd024f40aea110112c1d068a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=435778770, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NzanpYVu0lo",
        "colab_type": "text"
      },
      "source": [
        "# Import files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpQZjmyjuw-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import files\n",
        "x_train = pd.read_pickle('/content/drive/My Drive/cis522/project/x_train.pkl')\n",
        "x_test = pd.read_pickle('/content/drive/My Drive/cis522/project/x_test.pkl')\n",
        "x_val = pd.read_pickle('/content/drive/My Drive/cis522/project/x_val.pkl')\n",
        "y_train = pd.read_pickle('/content/drive/My Drive/cis522/project/y_train.pkl')\n",
        "y_test = pd.read_pickle('/content/drive/My Drive/cis522/project/y_test.pkl')\n",
        "y_val = pd.read_pickle('/content/drive/My Drive/cis522/project/y_val.pkl')\n",
        "\n",
        "with open('/content/drive/My Drive/cis522/project/diagnoses.pkl', 'rb') as dict_file:\n",
        "    diagnoses_dict = pickle.load(dict_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdem6znUu3Da",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfdF2s3VRuqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.str.replace(\"\\n\", \" \").str.split(\"<SECTION>\")\n",
        "x_test = x_test.str.replace(\"\\n\", \" \").str.split(\"<SECTION>\")\n",
        "x_val = x_val.str.replace(\"\\n\", \" \").str.split(\"<SECTION>\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2dQYz_FU5BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(document):\n",
        "  ids = []\n",
        "  mask = []\n",
        "  for sent in document:\n",
        "    encoded = tokenizer.encode_plus(sent, max_length=256, add_special_tokens=True, pad_to_max_length=True, return_token_type_ids=False)\n",
        "    ids.append(torch.tensor(encoded['input_ids']))\n",
        "    mask.append(torch.tensor(encoded['attention_mask']))\n",
        "  ids = torch.stack(ids)\n",
        "  mask = torch.stack(mask)\n",
        "  return (ids, mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp_fZEVCD3oR",
        "colab_type": "code",
        "outputId": "f2c2daf7-cd48-4dff-810c-c71d90fdae8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_ids, x_mask = tokenize(x_train.iloc[0])\n",
        "x_ids, x_mask = x_ids.to(device), x_mask.to(device)\n",
        "x_embed = model(x_ids, token_type_ids=None, attention_mask=x_mask) # second to last hidden layer - best for classification? https://github.com/BramVanroy/bert-for-inference/blob/master/introduction-to-bert.ipynb\n",
        "print(x_embed[1].shape) # should be the hidden layer"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([17, 256])\n",
            "torch.Size([17, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8oflDipvtcu",
        "colab_type": "text"
      },
      "source": [
        "# Dataset/dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz3rb8J7we1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build a mapping from y (diagnostic codes) to classes based on the number of elements in the classs\n",
        "def buildMap(y, diagnosis_dict, min_class_size, start_index = 1):\n",
        "  codeToClassMap = {}\n",
        "  classToCodeMap = {}\n",
        "  # i = 0 should be reserved for padding\n",
        "  # i = 1 should be reserved for classes to catch the rest\n",
        "  i = start_index\n",
        "  for row in y:\n",
        "    for code in row:\n",
        "      if diagnosis_dict[code] >= min_class_size:\n",
        "        if code not in codeToClassMap:\n",
        "          codeToClassMap[code] = i\n",
        "          classToCodeMap[i] = code\n",
        "          i += 1\n",
        "  return (codeToClassMap, classToCodeMap, i+1) # return(map, class_size). add 2 extra classes\n",
        "  \n",
        "(codeToClassMap, classToCodeMap, num_classes) = buildMap(pd.concat([y_train, y_test, y_val]), diagnoses_dict, 200)\n",
        "\n",
        "# create the dataset\n",
        "# all classes not in codeToClassMap should be lumped into a single class 0\n",
        "class MIMICDataset(Dataset):\n",
        "    def __init__(self, x, y, codeToClassMap, classToCodeMap, num_classes):\n",
        "      # import maps\n",
        "      self.num_classes = num_classes\n",
        "      self.codeToClassMap = codeToClassMap\n",
        "      self.classToCodeMap = classToCodeMap\n",
        "\n",
        "      # if there are y's, i.e. we are training\n",
        "      if y is not None:\n",
        "        # map the y's to a class\n",
        "        self.y = self.mapCodeToClasses(y)\n",
        "        self.frequency_dict = self.buildDict(self.y)\n",
        "\n",
        "      # for the x's\n",
        "      self.x = x\n",
        "\n",
        "      # weights for the loss function\n",
        "      self.classLossWeights = self.getClassLossWeights()\n",
        "      pass\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.x)\n",
        "      pass\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # get the embeddings\n",
        "      with torch.no_grad():    \n",
        "        x_id, x_mask = tokenize(self.x.iloc[idx])\n",
        "        x_id, x_mask = x_id.to(device), x_mask.to(device)\n",
        "        x_embed = model(x_id, token_type_ids=None, attention_mask=x_mask)[1] # second to last hidden layer - best for classification? https://github.com/BramVanroy/bert-for-inference/blob/master/introduction-to-bert.ipynb\n",
        "        length = torch.tensor(len(x_embed))\n",
        "\n",
        "\n",
        "      # get y into one-hot\n",
        "      if self.y is not None:\n",
        "        y = torch.tensor(self.y[idx])\n",
        "        y = y.unsqueeze(0)\n",
        "        y = torch.zeros(y.size(0), self.num_classes).scatter(1, y, 1.)[0]\n",
        "        return x_embed, y, length\n",
        "\n",
        "      return x_embed, length\n",
        "      pass\n",
        "\n",
        "\n",
        "    # get the class of of ys\n",
        "    def mapCodeToClasses(self, y):\n",
        "      classes = []\n",
        "      for i, row in enumerate(y):\n",
        "        classes.append([])\n",
        "        for code in row:\n",
        "          if code in self.codeToClassMap: # this means we are trying to predict the class\n",
        "            classes[i].append(self.codeToClassMap[code])\n",
        "          elif 1 not in classes[i]: # we are not trying to predict the class - add a class 1 if not yet in the list\n",
        "            classes[i].append(1)\n",
        "      return classes\n",
        "\n",
        "\n",
        "    # Build a dictionary of frequencies. Add 1 to the dict for each class appearance.\n",
        "    def buildDict(self, y):\n",
        "      frequency_dict = {}\n",
        "      # iterate through all items in y\n",
        "      for row in y:\n",
        "        for class_label in row:\n",
        "          # if the class is in the map, add the class frequency to the dictionary\n",
        "          if class_label in frequency_dict:\n",
        "            frequency_dict[class_label] += 1\n",
        "          else:\n",
        "            frequency_dict[class_label] = 1\n",
        "      return frequency_dict\n",
        "      pass\n",
        "\n",
        "\n",
        "    def getClassLossWeights(self):\n",
        "      weights = np.empty(self.num_classes, dtype='float32')\n",
        "      for key, value in self.frequency_dict.items():\n",
        "        weights[key] = 1 / value\n",
        "      return torch.tensor(weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03zWq-2T5o3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate(batch):\n",
        "    batch_size = len(batch)\n",
        "    batch_split = list(zip(*batch))\n",
        "    seqs, targs, lengths = batch_split[0], batch_split[1], batch_split[2]\n",
        "    seqs = pad_sequence(seqs, batch_first=True)\n",
        "    targs = torch.stack(targs)\n",
        "    lengths = torch.stack(lengths)\n",
        "    return seqs, targs, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SONBW4e_7WSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = MIMICDataset(x_train, y_train, codeToClassMap, classToCodeMap, num_classes)\n",
        "train_loader = DataLoader(train_dataset, collate_fn=collate, batch_size=32)\n",
        "\n",
        "val_dataset = MIMICDataset(x_val, y_val, codeToClassMap, classToCodeMap, num_classes)\n",
        "val_loader = DataLoader(val_dataset, collate_fn=collate, batch_size=32)\n",
        "\n",
        "test_dataset = MIMICDataset(x_val, y_val, codeToClassMap, classToCodeMap, num_classes)\n",
        "test_loader = DataLoader(val_dataset, collate_fn=collate, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf-zjnAh-APW",
        "colab_type": "code",
        "outputId": "c3adbbdd-dc6a-4cf8-8c19-4b5142a011d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for batch in val_loader:\n",
        "  x, y, lengths = batch\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  print(lengths)\n",
        "  break"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 35, 768])\n",
            "torch.Size([32, 475])\n",
            "tensor([23, 18, 16, 21, 19, 20, 23,  9, 21, 32, 15, 34, 18, 23,  7, 22, 25, 13,\n",
            "        17, 22, 17, 20, 10, 18, 20, 22, 11, 35, 12, 22, 14, 22])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyKX7QksH4hD",
        "colab_type": "text"
      },
      "source": [
        "# Logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O60TAZoDF851",
        "colab_type": "text"
      },
      "source": [
        "#Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtGrUSe_F7nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_f1(target, pred):\n",
        "    ones = torch.ones(target.shape).to(device)\n",
        "    tp = ((pred == target).float() == target).float().sum().item()\n",
        "    fp = ((pred == (ones - target)).float() == (ones - target)).float().sum().item()\n",
        "    fn = ((pred == (ones - target)).float() == target).float().sum().item()\n",
        "    if tp is 0 and fp is 0:\n",
        "      precision = 0\n",
        "    else:\n",
        "      precision = tp / (tp + fp)\n",
        "    if tp is 0 and fn is 0:\n",
        "      recall = 0\n",
        "    else:\n",
        "      recall = tp / (tp + fn)\n",
        "    if precision is 0 and recall is 0:\n",
        "      f1 is 0\n",
        "    else:\n",
        "      f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "# set up the train/test infrastructure\n",
        "# ALERT ALERT ALERT: make sure you have a folder named checkpoints within your hw directory\n",
        "def train_model(iterator, model, criterion, optimizer, num_epochs, log_file=None):\n",
        "  # set up logging infrastructure\n",
        "  if log_file:\n",
        "    logger = SummaryWriter(os.path.join(ROOT_LOG_DIR, log_file))\n",
        "\n",
        "  # for each epoch\n",
        "  for e in range(num_epochs):\n",
        "\n",
        "    # save the model every 10 epochs. \n",
        "    # to reload, execute the following:\n",
        "    #   model = MODEL()\n",
        "    #   model.load_state_dict(torch.load(PATH))\n",
        "    #   model.eval()\n",
        "    if log_file and e is not 0 and e % 10 is 0 and e is not num_epochs - 1:\n",
        "      CHECKPOINT_PATH = '/content/drive/My Drive/cis522/HW4/checkpoints'\n",
        "      CHECKPOINT_PATH = os.path.join(CHECKPOINT_PATH,log_file+'_epoch'+str(e))\n",
        "      torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
        "\n",
        "    # define loss and f1 for this epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    # for each batch\n",
        "    for idx, batch in enumerate(iterator):\n",
        "      if (idx % 50 == 0):\n",
        "        print(\"Epoch: \"+str(e)+\"\\t on batch \"+str(idx) + \" of \"+str(len(iterator)))\n",
        "\n",
        "      # zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # get the batch\n",
        "      x, y, lengths = batch\n",
        "      x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
        "\n",
        "      # run the model\n",
        "      output = model(x, lengths)\n",
        "\n",
        "      # calculate the loss\n",
        "      if (output.dim() == 1 ):\n",
        "        output.unsqueeze(0)\n",
        "      loss = criterion(output, y)\n",
        "\n",
        "      # backprop\n",
        "      loss.backward()\n",
        "\n",
        "      # update step\n",
        "      optimizer.step()\n",
        "\n",
        "      # update the loss and accuracy\n",
        "      epoch_loss += loss.data.item()\n",
        "      precision, recall, f1 = compute_f1(y.detach(), torch.round(torch.sigmoid(output.detach())))\n",
        "      epoch_f1 += f1\n",
        "\n",
        "      # clean up for memory\n",
        "      del batch\n",
        "      del x\n",
        "      del y\n",
        "      del lengths\n",
        "      del output\n",
        "      del loss\n",
        "      while (gc.collect() != 0):\n",
        "        x = 1\n",
        "        del x\n",
        "      \n",
        "\n",
        "    # print and log\n",
        "    epoch_loss = epoch_loss / (idx+1)\n",
        "    epoch_f1 = epoch_f1 / (idx+1)\n",
        "\n",
        "    print('Epoch:', e, '\\tLoss:', epoch_loss, '\\tF1:', epoch_f1)\n",
        "    if log_file:\n",
        "      logger.add_scalar('loss', epoch_loss, e)\n",
        "      logger.add_scalar('f1', epoch_f1, e)\n",
        "\n",
        "  # save the final model\n",
        "  if log_file:\n",
        "    CHECKPOINT_PATH = '/content/drive/My Drive/cis522/project/models/checkpoints'\n",
        "    CHECKPOINT_PATH = os.path.join(CHECKPOINT_PATH,log_file+'_epoch'+str(e)+'_final')\n",
        "    torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
        "  pass \n",
        "\n",
        "# haven't run - still need to debug.\n",
        "# def test_model(iterator, model, returnResults = False):\n",
        "#   with torch.no_grad():\n",
        "#     labels = []\n",
        "#     targets = []\n",
        "#     for idx, batch in enumerate(iterator):\n",
        "#       # get the batch\n",
        "#       (text, text_lengths), score = batch\n",
        "#       text, text_lengths, score = text.to(device), text_lengths.to(device), score.to(device)\n",
        "\n",
        "#       # run the model\n",
        "#       output = model(text, text_lengths)\n",
        "#       labels = labels + (torch.argmax(output, dim=1)).tolist()\n",
        "#       targets = targets + score.tolist()\n",
        "    \n",
        "#     print(f1_score(labels, targets, average='macro'))\n",
        "#     if returnResults:\n",
        "#       return targets, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPdRtCACwGIo",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR4NKwY3ZBVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "from torch.nn.utils.rnn import pack_padded_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6roVbm-YvUDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MIMICClassifier(nn.Module):\n",
        "  \"\"\" \n",
        "  Mimic Classifier\n",
        "\n",
        "  Parameters: \n",
        "  mode (string): Type of recurrent layer being used. Types are ['rnn', 'lstm', 'gru', 'bilstm']\n",
        "  output_size (int): Size of the last layer for classification (hint: how many classes do you have?)\n",
        "  hidden_size (int): Length of your hidden state vector\n",
        "  vocab_size (int): Length of your vocab (can get this by doing len(TEXT.vocab))\n",
        "  embedding_length (int): Dimension of your word embedding vector (hint: look at part 2c)\n",
        "  word_embeddings (Tensor): All of the word embeddings generated. Can get this from TEXT.vocab.vectors\n",
        "  \"\"\"\n",
        "  def __init__(self, mode, output_size, embedding_length, hidden_size):\n",
        "    super(MIMICClassifier, self).__init__()\n",
        "\n",
        "    self.mode = mode\n",
        "\n",
        "    if mode is 'rnn':\n",
        "      self.recurrent_layer = nn.RNN(embedding_length, hidden_size, nonlinearity='relu', batch_first=True)\n",
        "    elif mode is 'lstm':\n",
        "      self.recurrent_layer = nn.LSTM(embedding_length, hidden_size, batch_first=True)\n",
        "    elif mode is 'gru':\n",
        "      self.recurrent_layer = nn.GRU(embedding_length, hidden_size, batch_first=True)\n",
        "    elif mode is 'bilstm':\n",
        "      self.recurrent_layer = nn.LSTM(embedding_length, hidden_size, bidirectional=True, batch_first=True)\n",
        "    else:\n",
        "      raise ValueError(\"Choose a mode from - rnn / lstm / gru / bilstm\")\n",
        "    \n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def getHidden(self, sequence):\n",
        "    if self.mode is 'rnn':\n",
        "      output, hidden = sequence\n",
        "      return hidden.squeeze()\n",
        "    elif self.mode is 'lstm':\n",
        "      output, (hidden, cell) = sequence\n",
        "      return hidden.squeeze()\n",
        "    elif self.mode is 'gru':\n",
        "      output, hidden = sequence\n",
        "      return hidden.squeeze()\n",
        "    elif self.mode is 'bilstm':\n",
        "      output, (hidden, cell) = sequence\n",
        "      hidden = (hidden[0, :, :] + hidden[1, : , :])\n",
        "      return hidden.squeeze()\n",
        "\n",
        "  def forward(self, embedding_sequence, lengths):\n",
        "    packed = pack_padded_sequence(embedding_sequence, lengths, enforce_sorted = False, batch_first=True)\n",
        "    sequence = self.recurrent_layer(packed)\n",
        "    hidden = self.getHidden(sequence)\n",
        "    return self.fc(hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wpVi_P_Fywd",
        "colab_type": "code",
        "outputId": "16517281-65d8-4456-eb63-c462adbde500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "rnn = MIMICClassifier(\n",
        "    mode='rnn', \n",
        "    output_size=num_classes, \n",
        "    hidden_size=512, \n",
        "    embedding_length=768, \n",
        "    )\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "  rnn = nn.DataParallel(rnn)\n",
        "rnn.to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(weight=val_dataset.getClassLossWeights().to(device))\n",
        "optimizer = torch.optim.Adam(rnn.parameters()) # need low learning rate - gradients are funky here.\n",
        "train_model(val_loader, rnn, criterion, optimizer, 5)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\t on batch 0 of 294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-160-84cdd77f29f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetClassLossWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need low learning rate - gradients are funky here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-158-4d8edaf3c7de>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(iterator, model, criterion, optimizer, num_epochs, log_file)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# for each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\t on batch \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" of \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-151-99765a0998a0>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mx_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mx_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# second to last hidden layer - best for classification? https://github.com/BramVanroy/bert-for-inference/blob/master/introduction-to-bert.ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3NdOljpZP9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del rnn\n",
        "while (gc.collect() != 0):\n",
        "  x = 1\n",
        "  del x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aio6hbhVaA9R",
        "colab_type": "code",
        "outputId": "35b03043-c3c3-4ca3-f71a-ba3072880400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x=1\n",
        "del x\n",
        "x = gc.collect()\n",
        "print(x)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWF3dxs3aBpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}